import os
import json
import math
import glob
from collections import Counter
from tqdm import tqdm

def calculate_entropy_metrics(counter):
    """
    è®¡ç®—é¦™å†œç†µåŠå…¶å½’ä¸€åŒ–å€¼
    """
    total_count = sum(counter.values())
    unique_count = len(counter)
    
    # 1. å¤„ç†ç©ºæ•°æ®æˆ–åªæœ‰1ç§æƒ…å†µçš„è¾¹ç•Œæ¡ä»¶
    if total_count == 0 or unique_count <= 1:
        return 0.0, 0.0, unique_count
    
    # 2. è®¡ç®—åŸå§‹é¦™å†œç†µ H(X)
    raw_entropy = 0.0
    for count in counter.values():
        p_x = count / total_count
        raw_entropy -= p_x * math.log2(p_x)
    
    # 3. è®¡ç®—æœ€å¤§å¯èƒ½çš„ç†µ H_max = log2(N)
    max_entropy = math.log2(unique_count)
    
    # 4. è®¡ç®—å½’ä¸€åŒ–ç†µ (Efficiency) = H(X) / H_max
    normalized_entropy = raw_entropy / max_entropy if max_entropy > 0 else 0.0
    
    return raw_entropy, normalized_entropy, unique_count

def get_diversity_level(norm_entropy):
    """æ ¹æ®å½’ä¸€åŒ–ç†µç»™å‡ºç®€å•çš„é˜ˆå€¼è¯„ä»·"""
    if norm_entropy < 0.3:
        return "ä½ (Low) - æ¨¡å¼éå¸¸å›ºå®š/å•ä¸€"
    elif norm_entropy < 0.7:
        return "ä¸­ (Moderate) - å­˜åœ¨ä¸»è¦æ¨¡å¼ï¼Œå…¼é¡¾å¤šæ ·æ€§"
    else:
        return "é«˜ (High) - åˆ†å¸ƒéå¸¸å‡åŒ€/å‘æ•£ (æˆ–è¿‡äºæ‚ä¹±)"

def analyze_tool_usage(root_paths):
    individual_tool_counter = Counter() 
    tool_chain_counter = Counter()      
    
    all_json_files = []

    # --- 1. èšåˆå¤šä¸ªæ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶ ---
    print(f"æ­£åœ¨æ‰«æ {len(root_paths)} ä¸ªæ•°æ®ç›®å½•...")
    for path in root_paths:
        if not os.path.exists(path):
            print(f"âš ï¸ è­¦å‘Š: è·¯å¾„ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡ -> {path}")
            continue
            
        search_pattern = os.path.join(path, "**", "*.json")
        # é€’å½’æŸ¥æ‰¾è¯¥ç›®å½•ä¸‹çš„æ‰€æœ‰json
        files = glob.glob(search_pattern, recursive=True)
        print(f"  - ç›®å½• {path}: å‘ç° {len(files)} ä¸ªæ–‡ä»¶")
        all_json_files.extend(files)

    print(f"æ€»è®¡æ‰¾åˆ° {len(all_json_files)} ä¸ªJSONæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
    valid_files_count = 0

    # --- 2. éå†å¤„ç† ---
    for file_path in tqdm(all_json_files, desc="Processing files"):
        
        # === è·³è¿‡ç‰¹å®šæ–‡ä»¶ ===
        file_name = os.path.basename(file_path)
        if file_name == "batch_summary.json" or file_name == "question_info.json":
            continue
        # ===============================================

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ–°æ ¼å¼ï¼šä» response æ•°ç»„ä¸­æå–å·¥å…·ä½¿ç”¨ä¿¡æ¯
            response_steps = data.get("response", [])
            if not response_steps:
                continue
                
            current_chain = []
            
            # éå† response æ•°ç»„ä¸­çš„æ¯ä¸ªæ­¥éª¤
            for step_item in response_steps:
                # step_item æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œkey æ˜¯ "step1", "step2" ç­‰
                # value æ˜¯åŒ…å« "cot" å’Œ "coa" çš„å­—å…¸
                for step_key, step_content in step_item.items():
                    # è·å– coa æ•°ç»„
                    coa_list = step_content.get("coa", [])
                    if not coa_list:
                        continue
                    
                    # éå†è¯¥æ­¥éª¤ä¸­çš„æ‰€æœ‰ action-observation å¯¹
                    for coa_item in coa_list:
                        action = coa_item.get("action", {})
                        if not action:
                            continue
                        
                        tool_name = action.get("name", "").strip()
                        if not tool_name:
                            continue
                        
                        # ç»Ÿè®¡å•ä¸ªå·¥å…·ä½¿ç”¨
                        individual_tool_counter[tool_name] += 1
                        # æ·»åŠ åˆ°å·¥å…·é“¾
                        current_chain.append(tool_name)
            
            if current_chain:
                chain_signature = " -> ".join(current_chain)
                tool_chain_counter[chain_signature] += 1
                valid_files_count += 1
                
        except Exception as e:
            # print(f"Error reading {file_path}: {e}") 
            continue

    # --- 3. è®¡ç®—æŒ‡æ ‡ ---
    
    # æŒ‡æ ‡ A: å•ä¸ªå·¥å…·åˆ†å¸ƒ
    t_raw, t_norm, t_unique = calculate_entropy_metrics(individual_tool_counter)
    
    # æŒ‡æ ‡ B: å·¥å…·é“¾ç»„åˆåˆ†å¸ƒ (æ ¸å¿ƒ)
    c_raw, c_norm, c_unique = calculate_entropy_metrics(tool_chain_counter)

    # --- 4. è¾“å‡ºæŠ¥å‘Š ---
    print("\n" + "="*60)
    print("ğŸ“Š å¤šæ•°æ®é›†å·¥å…·å¤šæ ·æ€§åˆ†ææŠ¥å‘Š (Normalized Analysis)")
    print("="*60)
    print(f"åŒ…å«ç›®å½•æ•°: {len(root_paths)}")
    print(f"æœ‰æ•ˆè½¨è¿¹æ•° (Valid Trajectories): {valid_files_count}")
    print("-" * 60)
    
    print(f"ã€æŒ‡æ ‡ Aã€‘ å•ä¸ªå·¥å…·ä½¿ç”¨åˆ†å¸ƒ (Tool Usage Distribution)")
    print(f"  - å”¯ä¸€å·¥å…·æ•° (N):       {t_unique}")
    print(f"  - åŸå§‹ç†µ (Raw Entropy): {t_raw:.4f}")
    print(f"  - å½’ä¸€åŒ–ç†µ (0~1):       {t_norm:.4f}  [{get_diversity_level(t_norm)}]")
    
    print("-" * 60)
    
    print(f"ã€æŒ‡æ ‡ Bã€‘ å·¥å…·é“¾è·¯å¾„ç»„åˆ (Tool Chain Diversity) <--- æ ¸å¿ƒå…³æ³¨")
    print(f"  - å”¯ä¸€è·¯å¾„ç»„åˆæ•° (N):   {c_unique}")
    print(f"  - åŸå§‹ç†µ (Raw Entropy): {c_raw:.4f}")
    print(f"  - å½’ä¸€åŒ–ç†µ (0~1):       {c_norm:.4f}  [{get_diversity_level(c_norm)}]")
    
    print("-" * 60)
    print("ğŸ’¡ é˜ˆå€¼è§£è¯»å‚è€ƒ:")
    print("   [0.0 - 0.3]: é›†ä¸­åº¦é«˜ã€‚æ¨¡å‹æ€»æ˜¯å€¾å‘äºä½¿ç”¨æŸ1-2ç§ç‰¹å®šçš„è§£å†³è·¯å¾„ã€‚")
    print("   [0.3 - 0.7]: å¹³è¡¡çŠ¶æ€ã€‚æ—¢æœ‰ä¸»æµçš„è§£å†³å¥—è·¯ï¼Œä¹Ÿæœ‰å¤„ç†é•¿å°¾é—®é¢˜çš„å˜ä½“ã€‚")
    print("   [0.7 - 1.0]: ç¦»æ•£åº¦é«˜ã€‚å‡ ä¹æ²¡æœ‰å›ºå®šçš„å¥—è·¯ï¼Œæ¯æ¡æ•°æ®çš„è§£å†³è·¯å¾„éƒ½ä¸åŒã€‚")
    print("="*60)
    
    print("\nTop 5 æœ€å¸¸ç”¨çš„å·¥å…·ç»„åˆ (åŠå…¶å æ¯”):")
    total_chains = sum(tool_chain_counter.values())
    if total_chains > 0:
        for chain, count in tool_chain_counter.most_common(5):
            ratio = (count / total_chains) * 100
            print(f"  {ratio:5.1f}% | [{chain}]")
    else:
        print("  (æ— æ•°æ®)")
    print("="*60)

if __name__ == "__main__":
    # é…ç½®ä½ çš„æ–‡ä»¶å¤¹åˆ—è¡¨
    DATA_PATHS = [
        "/raid/data/ly/data/dataset/data",
        # "/raid/data/ly/data2",
        # "/raid/data/ly/data3",
        # ä½ å¯ä»¥ç»§ç»­æ·»åŠ æ›´å¤šè·¯å¾„...
    ]
    
    analyze_tool_usage(DATA_PATHS)

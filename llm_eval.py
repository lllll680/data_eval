import os
import json
import glob
import numpy as np
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM

# =================é…ç½®åŒºåŸŸ=================
DATA_DIRS = [
    "/data2/ly/dataset_eval/code_apply/",
#     "/data2/ly/dataset_eval/code_apply_2/",
#     "/data2/ly/dataset_eval/code_apply_3/"
]
MODEL_PATH = "/data2/Qwen/Qwen2.5-72B-Instruct"

# ä¸ºäº†æ¼”ç¤ºï¼Œå¦‚æœæ•°æ®é‡å·¨å¤§ï¼Œå¯ä»¥è®¾ç½® SAMPLE_NUM åªè¯„ä¼°å‰Næ¡è¿›è¡ŒéªŒè¯
# è®¾ç½®ä¸º None åˆ™è¯„ä¼°æ‰€æœ‰æ•°æ®
SAMPLE_NUM = 50 
# =========================================

class AgentDataEvaluator:
    def __init__(self, data_dirs):
        self.data_dirs = data_dirs
        self.files = self._load_files()
        print(f"å…±æ‰¾åˆ° {len(self.files)} ä¸ªæ•°æ®æ–‡ä»¶ã€‚")

    def _load_files(self):
        files = []
        for d in self.data_dirs:
            # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ .json æ–‡ä»¶
            files.extend(glob.glob(os.path.join(d, "**", "*.json"), recursive=True))
        return files

    def get_data_content(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading {file_path}: {e}")
            return None

    # --- æŒ‡æ ‡ 1: å¹³å‡æ¨ç†æ­¥æ•° ---
    def calculate_average_steps(self):
        step_counts = []
        for file_path in tqdm(self.files, desc="è®¡ç®—å¹³å‡æ­¥æ•°"):
            data = self.get_data_content(file_path)
            if data and "execution_records" in data:
                # è¿‡æ»¤æ‰ç©ºçš„ recordï¼Œåªè®¡ç®—å®é™…æ­¥æ•°
                records = data["execution_records"]
                if isinstance(records, list):
                    step_counts.append(len(records))
        
        if not step_counts:
            return 0, 0
            
        avg_steps = np.mean(step_counts)
        max_steps = np.max(step_counts)
        return avg_steps, max_steps

class QwenJudge:
    def __init__(self, model_path):
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path} ... (å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        # ä½¿ç”¨ auto device map è‡ªåŠ¨åˆ†é…æ˜¾å­˜ï¼Œéœ€ä¿è¯æ˜¾å­˜è¶³å¤Ÿ
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, 
            device_map="auto", 
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        self.model.eval()

    def construct_evaluation_prompt(self, data):
        """
        æ„å»º LLM-as-a-Judge çš„ Promptã€‚
        è¿™æ˜¯è¯„ä¼°é€»è¾‘è¿è´¯æ€§å’Œæ•°æ®è´¨é‡çš„æ ¸å¿ƒã€‚
        """
        question = data.get("question", "")
        records = data.get("execution_records", [])
        
        # å°†è½¨è¿¹æ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„æ–‡æœ¬
        trajectory_text = ""
        for i, rec in enumerate(records):
            trajectory_text += f"Step {i+1}:\n"
            trajectory_text += f"  Reasoning: {rec.get('reasoning', '')}\n"
            trajectory_text += f"  Tool Call: {rec.get('tool_name', '')} -> {json.dumps(rec.get('tool_request', {}), ensure_ascii=False)}\n"
            trajectory_text += f"  Tool Output: {str(rec.get('tool_response', ''))[:200]}...\n" # æˆªæ–­è¿‡é•¿çš„è¾“å‡º
            trajectory_text += "-" * 20 + "\n"

        # === æ ¸å¿ƒ Prompt è®¾è®¡ ===
        # å‚è€ƒäº† G-Eval å’Œ AgentBench çš„æ‰“åˆ†é€»è¾‘
        prompt = f"""
### Role
You are an expert AI Assistant Evaluator. Your task is to evaluate the quality of an Agent's execution trajectory based on a specific User Question.

### Input Data
**User Question:** 
{question}

**Agent Execution Trajectory:**
{trajectory_text}

### Evaluation Criteria
Please score the trajectory on a scale of 1 to 10 for the following three dimensions. Then calculate a Weighted Final Score.

1. **Logical Coherence (Weight: 0.4)**
   - Does the reasoning in each step logically follow from the previous step and tool outputs?
   - Is the plan clear, or is the agent randomly trying tools?
   - Are there any contradictions between the reasoning and the action?

2. **Tool Usage Validity (Weight: 0.3)**
   - Are the selected tools appropriate for the current sub-goal?
   - Are the parameters generated for the tools correct and reasonable?

3. **Goal Efficiency (Weight: 0.3)**
   - Did the agent make progress towards solving the user's question?
   - Is the trajectory concise, or does it contain unnecessary redundant steps?

### Output Format
You must output a valid JSON object strictly following this format, with no extra text:
{{
    "analysis": "Brief justification for the scores...",
    "scores": {{
        "logical_coherence": <float 1-10>,
        "tool_usage_validity": <float 1-10>,
        "goal_efficiency": <float 1-10>
    }},
    "weighted_final_score": <float 1-10>
}}
"""
        return prompt

    def evaluate_one(self, data):
        prompt = self.construct_evaluation_prompt(data)
        messages = [
            {"role": "system", "content": "You are a helpful and rigorous AI evaluator."},
            {"role": "user", "content": prompt}
        ]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=512,
                temperature=0.2, # ä½æ¸©åº¦ä¿è¯è¯„åˆ†ç¨³å®šæ€§
                top_p=0.9
            )
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
        return self._parse_json(response)

    def _parse_json(self, response):
        # ç®€å•çš„æ¸…ç†å’Œè§£æé€»è¾‘ï¼Œé˜²æ­¢æ¨¡å‹è¾“å‡º ```json ... ```
        clean_str = response.replace("```json", "").replace("```", "").strip()
        try:
            return json.loads(clean_str)
        except:
            print(f"Warning: Failed to parse JSON. Response was: {clean_str[:50]}...")
            return None

# ================= ä¸»æ‰§è¡Œé€»è¾‘ =================
if __name__ == "__main__":
    # 1. ç»Ÿè®¡è®¡ç®—
    evaluator = AgentDataEvaluator(DATA_DIRS)
    avg_steps, max_steps = evaluator.calculate_average_steps()
    
    print("\n" + "="*40)
    print(f"ğŸ“Š åŸºç¡€ç»Ÿè®¡ (Basic Statistics)")
    print("="*40)
    print(f"å¹³å‡æ¨ç†æ­¥æ•° (Avg Steps): {avg_steps:.2f}")
    print(f"æœ€å¤§æ¨ç†æ­¥æ•° (Max Steps): {max_steps}")
    print("="*40 + "\n")

    # 2. æ¨¡å‹æ‰“åˆ† (å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œè¯·æ³¨é‡Šæ‰è¿™éƒ¨åˆ†)
    # åªæœ‰å½“æ ·æœ¬é‡ > 0 æ—¶æ‰è¿è¡Œ
    if evaluator.files:
        judge = QwenJudge(MODEL_PATH)
        
        scores_log = []
        files_to_eval = evaluator.files[:SAMPLE_NUM] if SAMPLE_NUM else evaluator.files
        
        print(f"å¼€å§‹ä½¿ç”¨ Qwen-72B è¿›è¡Œæ‰“åˆ†ï¼Œå…± {len(files_to_eval)} æ¡æ•°æ®...")
        
        for file_path in tqdm(files_to_eval):
            data = evaluator.get_data_content(file_path)
            if not data: continue
            
            result = judge.evaluate_one(data)
            if result:
                scores_log.append(result["weighted_final_score"])
        
        if scores_log:
            avg_score = np.mean(scores_log)
            print("\n" + "="*40)
            print(f"ğŸ§  æ¨¡å‹è¯„åˆ†ç»“æœ (LLM-as-a-Judge Evaluation)")
            print("="*40)
            print(f"è¯„ä¼°æ¨¡å‹: Qwen2.5-72B-Instruct")
            print(f"è¯„ä¼°æ ·æœ¬æ•°: {len(scores_log)}")
            print(f"åŠ æƒç»¼åˆå¾—åˆ† (Weighted Final Score): {avg_score:.2f} / 10.0")
            print("="*40)
        else:
            print("æœªç”Ÿæˆæœ‰æ•ˆè¯„åˆ†ã€‚")

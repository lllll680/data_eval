import os
import json
import math
import glob
from collections import Counter
from tqdm import tqdm  # å¦‚æœæ²¡æœ‰å®‰è£…ï¼Œå¯ä»¥ä½¿ç”¨ pip install tqdmï¼Œæˆ–è€…åˆ é™¤ç›¸å…³ä»£ç 

def calculate_shannon_entropy(counter):
    """
    è®¡ç®—é¦™å†œç†µ (Shannon Entropy)
    H(X) = - sum(p(x) * log2(p(x)))
    """
    total_count = sum(counter.values())
    if total_count == 0:
        return 0.0
    
    entropy = 0.0
    for count in counter.values():
        p_x = count / total_count
        entropy -= p_x * math.log2(p_x)
    
    return entropy

def analyze_tool_usage(root_path):
    # 1. åˆå§‹åŒ–è®¡æ•°å™¨
    individual_tool_counter = Counter() # è®°å½•å•ä¸ªå·¥å…·å‡ºç°çš„æ¬¡æ•° (ä¾‹å¦‚: "Search": 100)
    tool_chain_counter = Counter()      # è®°å½•å·¥å…·ç»„åˆè·¯å¾„çš„æ¬¡æ•° (ä¾‹å¦‚: "Search->Calculator": 50)
    
    # è·å–æ‰€æœ‰jsonæ–‡ä»¶è·¯å¾„ (é€’å½’æŸ¥æ‰¾)
    # å‡è®¾è·¯å¾„ç»“æ„æ˜¯ /data2/ly/dataset_eval/code_apply/ ä¸‹é¢çš„ä»»æ„å­æ–‡ä»¶å¤¹ä¸­
    search_pattern = os.path.join(root_path, "**", "*.json")
    json_files = glob.glob(search_pattern, recursive=True)
    
    print(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

    valid_files_count = 0

    # 2. éå†æ–‡ä»¶å¹¶æå–æ•°æ®
    for file_path in tqdm(json_files, desc="Processing files"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æå– execution_records
            records = data.get("execution_records", [])
            
            # å¦‚æœæ²¡æœ‰æ‰§è¡Œè®°å½•ï¼Œè·³è¿‡
            if not records:
                continue
                
            current_chain = []
            
            for step in records:
                # è·å– tool_nameï¼Œå¦‚æœä¸ºç©ºåˆ™æ ‡è®°ä¸º "unknown"
                t_name = step.get("tool_name", "").strip()
                if not t_name:
                    continue
                
                # ç»Ÿè®¡å•ä¸ªå·¥å…·
                individual_tool_counter[t_name] += 1
                current_chain.append(t_name)
            
            # ç»Ÿè®¡å·¥å…·é“¾ (å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²æˆ–å…ƒç»„ä½œä¸ºKey)
            if current_chain:
                # ä½¿ç”¨ " -> " è¿æ¥ï¼Œå½¢æˆå¦‚ "Search -> Calculator" çš„å­—ç¬¦ä¸²
                chain_signature = " -> ".join(current_chain)
                tool_chain_counter[chain_signature] += 1
                valid_files_count += 1
                
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            continue

    # 3. è®¡ç®—æŒ‡æ ‡
    
    # --- æŒ‡æ ‡ A: å”¯ä¸€å·¥å…·æ•°é‡ ---
    unique_tools = list(individual_tool_counter.keys())
    num_unique_tools = len(unique_tools)
    
    # --- æŒ‡æ ‡ B: å•ä¸ªå·¥å…·åˆ†å¸ƒç†µ (Tool Distribution Entropy) ---
    tool_distribution_entropy = calculate_shannon_entropy(individual_tool_counter)
    
    # --- æŒ‡æ ‡ C: å·¥å…·é“¾ç»„åˆç†µ (Tool Chain Entropy) --- 
    # è¿™ä¸ªæŒ‡æ ‡å¯¹åº”ä½ æƒ³è¦çš„ "ç»„åˆçš„é¢‘ç‡åˆ†å¸ƒï¼ˆç†µï¼‰"
    chain_entropy = calculate_shannon_entropy(tool_chain_counter)

    # 4. è¾“å‡ºç»“æœ
    print("\n" + "="*50)
    print("ğŸ“Š æ•°æ®é›†å·¥å…·å¤šæ ·æ€§åˆ†ææŠ¥å‘Š (Statistics Report)")
    print("="*50)
    print(f"æœ‰æ•ˆæ•°æ®æ¡æ•° (Valid Trajectories): {valid_files_count}")
    print(f"å”¯ä¸€å·¥å…·åç§°æ•°é‡ (Unique Tool Names): {num_unique_tools}")
    print("-" * 30)
    print(f"1. å•ä¸ªå·¥å…·åˆ†å¸ƒç†µ (Individual Tool Entropy): {tool_distribution_entropy:.4f}")
    print(f"2. å·¥å…·ç»„åˆè·¯å¾„ç†µ (Tool Chain Entropy):      {chain_entropy:.4f}  <-- æ ¸å¿ƒæŒ‡æ ‡")
    print("-" * 30)
    
    print("\nTop 5 æœ€å¸¸ç”¨çš„å·¥å…· (Most Frequent Tools):")
    for tool, count in individual_tool_counter.most_common(5):
        print(f"  - {tool}: {count}")

    print("\nTop 5 æœ€å¸¸ç”¨çš„å·¥å…·ç»„åˆ (Most Frequent Chains):")
    for chain, count in tool_chain_counter.most_common(5):
        print(f"  - [{chain}]: {count}")
    print("="*50)

if __name__ == "__main__":
    # é…ç½®ä½ çš„æ•°æ®è·¯å¾„
    DATA_PATH = "/data2/ly/dataset_eval/code_apply/"
    
    if os.path.exists(DATA_PATH):
        analyze_tool_usage(DATA_PATH)
    else:
        print(f"è·¯å¾„ä¸å­˜åœ¨: {DATA_PATH}")

import os
import json
import math
import glob
from collections import Counter
from tqdm import tqdm

def calculate_entropy_metrics(counter):
    """
    è®¡ç®—é¦™å†œç†µåŠå…¶å½’ä¸€åŒ–å€¼
    Returns:
        raw_entropy: åŸå§‹é¦™å†œç†µ (bits)
        normalized_entropy: å½’ä¸€åŒ–ç†µ (0~1), ä¾¿äºè·¨æ•°æ®é›†æ¯”è¾ƒ
        unique_count: ç±»åˆ«æ€»æ•° (N)
    """
    total_count = sum(counter.values())
    unique_count = len(counter)
    
    # 1. å¤„ç†ç©ºæ•°æ®æˆ–åªæœ‰1ç§æƒ…å†µçš„è¾¹ç•Œæ¡ä»¶
    if total_count == 0 or unique_count <= 1:
        return 0.0, 0.0, unique_count
    
    # 2. è®¡ç®—åŸå§‹é¦™å†œç†µ H(X)
    raw_entropy = 0.0
    for count in counter.values():
        p_x = count / total_count
        raw_entropy -= p_x * math.log2(p_x)
    
    # 3. è®¡ç®—æœ€å¤§å¯èƒ½çš„ç†µ H_max = log2(N)
    max_entropy = math.log2(unique_count)
    
    # 4. è®¡ç®—å½’ä¸€åŒ–ç†µ (Efficiency) = H(X) / H_max
    # èŒƒå›´ [0, 1]
    normalized_entropy = raw_entropy / max_entropy if max_entropy > 0 else 0.0
    
    return raw_entropy, normalized_entropy, unique_count

def get_diversity_level(norm_entropy):
    """æ ¹æ®å½’ä¸€åŒ–ç†µç»™å‡ºç®€å•çš„é˜ˆå€¼è¯„ä»· (ä»…ä¾›å‚è€ƒ)"""
    if norm_entropy < 0.3:
        return "ä½ (Low) - æ¨¡å¼éå¸¸å›ºå®š/å•ä¸€"
    elif norm_entropy < 0.7:
        return "ä¸­ (Moderate) - å­˜åœ¨ä¸»è¦æ¨¡å¼ï¼Œå…¼é¡¾å¤šæ ·æ€§"
    else:
        return "é«˜ (High) - åˆ†å¸ƒéå¸¸å‡åŒ€/å‘æ•£ (æˆ–è¿‡äºæ‚ä¹±)"

def analyze_tool_usage(root_path):
    individual_tool_counter = Counter() 
    tool_chain_counter = Counter()      
    
    search_pattern = os.path.join(root_path, "**", "*.json")
    json_files = glob.glob(search_pattern, recursive=True)
    
    print(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
    valid_files_count = 0

    for file_path in tqdm(json_files, desc="Processing files"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            records = data.get("execution_records", [])
            if not records:
                continue
                
            current_chain = []
            for step in records:
                t_name = step.get("tool_name", "").strip()
                if not t_name:
                    continue
                individual_tool_counter[t_name] += 1
                current_chain.append(t_name)
            
            if current_chain:
                chain_signature = " -> ".join(current_chain)
                tool_chain_counter[chain_signature] += 1
                valid_files_count += 1
                
        except Exception as e:
            # print(f"Error reading {file_path}: {e}") # æŠ¥é”™å¤ªå¤šæ—¶å¯æ³¨é‡Šæ‰
            continue

    # --- è®¡ç®—æŒ‡æ ‡ ---
    
    # 1. å•ä¸ªå·¥å…·åˆ†å¸ƒ
    t_raw, t_norm, t_unique = calculate_entropy_metrics(individual_tool_counter)
    
    # 2. å·¥å…·é“¾ç»„åˆåˆ†å¸ƒ (æ ¸å¿ƒ)
    c_raw, c_norm, c_unique = calculate_entropy_metrics(tool_chain_counter)

    # --- è¾“å‡ºæŠ¥å‘Š ---
    print("\n" + "="*60)
    print("ğŸ“Š æ•°æ®é›†å·¥å…·å¤šæ ·æ€§åˆ†ææŠ¥å‘Š (Normalized Analysis)")
    print("="*60)
    print(f"æœ‰æ•ˆè½¨è¿¹æ•° (Valid Trajectories): {valid_files_count}")
    print("-" * 60)
    
    print(f"ã€æŒ‡æ ‡ Aã€‘ å•ä¸ªå·¥å…·ä½¿ç”¨åˆ†å¸ƒ (Tool Usage Distribution)")
    print(f"  - å”¯ä¸€å·¥å…·æ•° (N):       {t_unique}")
    print(f"  - åŸå§‹ç†µ (Raw Entropy): {t_raw:.4f}")
    print(f"  - å½’ä¸€åŒ–ç†µ (0~1):       {t_norm:.4f}  [{get_diversity_level(t_norm)}]")
    
    print("-" * 60)
    
    print(f"ã€æŒ‡æ ‡ Bã€‘ å·¥å…·é“¾è·¯å¾„ç»„åˆ (Tool Chain Diversity) <--- æ ¸å¿ƒå…³æ³¨")
    print(f"  - å”¯ä¸€è·¯å¾„ç»„åˆæ•° (N):   {c_unique}")
    print(f"  - åŸå§‹ç†µ (Raw Entropy): {c_raw:.4f}")
    print(f"  - å½’ä¸€åŒ–ç†µ (0~1):       {c_norm:.4f}  [{get_diversity_level(c_norm)}]")
    
    print("-" * 60)
    print("ğŸ’¡ é˜ˆå€¼è§£è¯»å‚è€ƒ:")
    print("   [0.0 - 0.3]: é›†ä¸­åº¦é«˜ã€‚æ¨¡å‹æ€»æ˜¯å€¾å‘äºä½¿ç”¨æŸ1-2ç§ç‰¹å®šçš„è§£å†³è·¯å¾„ã€‚")
    print("   [0.3 - 0.7]: å¹³è¡¡çŠ¶æ€ã€‚æ—¢æœ‰ä¸»æµçš„è§£å†³å¥—è·¯ï¼Œä¹Ÿæœ‰å¤„ç†é•¿å°¾é—®é¢˜çš„å˜ä½“ã€‚")
    print("   [0.7 - 1.0]: ç¦»æ•£åº¦é«˜ã€‚å‡ ä¹æ²¡æœ‰å›ºå®šçš„å¥—è·¯ï¼Œæ¯æ¡æ•°æ®çš„è§£å†³è·¯å¾„éƒ½ä¸åŒã€‚")
    print("="*60)
    
    print("\nTop 5 æœ€å¸¸ç”¨çš„å·¥å…·ç»„åˆ (åŠå…¶å æ¯”):")
    total_chains = sum(tool_chain_counter.values())
    for chain, count in tool_chain_counter.most_common(5):
        ratio = (count / total_chains) * 100
        print(f"  {ratio:5.1f}% | [{chain}]")
    print("="*60)

if __name__ == "__main__":
    DATA_PATH = "/data2/ly/dataset_eval/code_apply/"
    
    if os.path.exists(DATA_PATH):
        analyze_tool_usage(DATA_PATH)
    else:
        print(f"è·¯å¾„ä¸å­˜åœ¨: {DATA_PATH}")
